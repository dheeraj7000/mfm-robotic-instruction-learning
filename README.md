# ğŸ¤– MFM-Robotic-Instruction-Learning
**Memory-Efficient Multimodal Fusion Model for Visionâ€“Language Robotic Reasoning in AI2-THOR**

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.1+-red)
![GPU](https://img.shields.io/badge/GPU-GTX1650%204GB-orange)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)

---

## ğŸ§  Overview
This repository contains the implementation of a **Multimodal Fusion Model (MFM)** designed for **memory-efficient robotic instruction learning** inside the **AI2-THOR environment**.  
The model jointly processes **visual frames** and **textual prompts** to generate:
- Next-step **robot actions**
- Contextual **VQA answers**
- Natural **language descriptions**
- Continuous **task progress estimates**

All experiments were conducted on a **single NVIDIA GeForce GTX 1650 (4 GB VRAM)**, demonstrating that full multimodal fine-tuning and inference are possible on consumer hardware without quantization or checkpointing.

---

## âš™ï¸ Key Features
- ğŸ§© **Gated Cross-Attention Fusion** â€” Aligns image and text features efficiently.  
- ğŸ”‹ **Low-Memory Footprint** â€” <4 GB VRAM even during fine-tuning.  
- ğŸ¤– **AI2-THOR Compatible** â€” Realistic robotic scenes with visual grounding.  
- ğŸ’¬ **Multitask Learning** â€” Supports Action, VQA, Description, and Progress modes.  
- ğŸ§  **Synthetic Data Generation** â€” Uses Mistral via LiteLLM for dataset creation.

---

## ğŸ§© Model Architecture
```

[ CLIP Encoder (frozen) ]
â†“ (512-D)
[ Linear Projection ]
â†“
[ Gated Cross-Attention ]
â†“
[ DistilGPT-2 Decoder ]
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
| ActionHead |  VQAHead   |  DescHead  | ProgHead   |
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
**Peak memory usage:** 3.7 GB on GTX 1650  
**Training time:** ~10 min/epoch on 550 samples

---

## ğŸ“¦ Repository Structure
```

mfm-robotic-instruction-learning/
â”‚
â”œâ”€â”€ multimodal_network.py                # MFM architecture definition
â”œâ”€â”€ finetune_multimodal.py               # Training pipeline
â”œâ”€â”€ inference_ai2thor.py                 # Inference & robot control in AI2-THOR
â”œâ”€â”€ synthetic_data_generator_litellm.py  # Synthetic dataset generator (LiteLLM + Mistral)
â”œâ”€â”€ synthetic_seq_generator.py           # Sequential scene data generator
â”‚
â”œâ”€â”€ checkpoints/                         # Model weights
â”œâ”€â”€ synthetic_dataset/                    # Frames + JSONL annotations
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ Memory_Efficient_MultimodalFusion_GTX1650_Report.docx
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## ğŸ§° Installation

```bash
git clone https://github.com/<your-username>/mfm-robotic-instruction-learning.git
cd mfm-robotic-instruction-learning
pip install -r requirements.txt
````

---

## ğŸ§  Synthetic Dataset Generation

Generate frames + annotations using Mistral via LiteLLM:

```bash
python synthetic_data_generator_litellm.py
```

For sequential episodes (plans + multi-step tasks):

```bash
python synthetic_seq_generator.py
```

Output:
`./synthetic_dataset/data.jsonl` with entries like:

```json
{
  "scene_id": "FloorPlan3",
  "mode": "vqa",
  "prompt": "What is visible on the table?",
  "question": "What is the only cutting tool visible?",
  "answer_text": "The ButterKnife.",
  "frame_path": "./synthetic_dataset/frames/FloorPlan3_0027.png"
}
```

---

## ğŸ‹ï¸ Model Fine-Tuning

Train the model on the generated dataset:

```bash
python finetune_multimodal.py
```

Expected logs:

```
=== Mode: DESCRIPTION ===
ğŸ“‰ Epoch 5 | Train loss: 0.25 | Val loss: 0.17
```

Results summary:

| Mode        | Train Loss | Val Loss | Observation           |
| ----------- | ---------- | -------- | --------------------- |
| Action      | 0.98       | 0.78     | Moderate learning     |
| VQA         | 0.47       | 0.35     | Good generalization   |
| Description | 0.25       | 0.17     | Excellent convergence |
| Progress    | 0.69       | 0.69     | Needs sequential data |

---

## ğŸ¤– Inference in AI2-THOR

```bash
python inference_ai2thor.py
```

Example Output:

```
ğŸ¤– Predicted action: PickupObject
ğŸ¬ Executing PickupObject in AI2-THOR...
âœ… Success: True
ğŸ‘ï¸ Visible objects now: ['Apple', 'Plate']
```

---

## ğŸ’¡ Hardware Configuration

| GPU                     | VRAM | CUDA | Driver |
| ----------------------- | ---- | ---- | ------ |
| NVIDIA GeForce GTX 1650 | 4 GB | 12.5 | 555.97 |

Peak training memory usage: **3.7 GB**
Batch size: **4**
No quantization or checkpointing used.

---

## ğŸ“š References

* [1] J. Li et al., *BLIP-2: Bootstrapped Language-Image Pre-Training*, arXiv:2301.12597, 2023.
* [2] J. Alayrac et al., *Flamingo: Visual Language Model for Few-Shot Learning*, NeurIPS 2022.
* [3] H. Liu et al., *Visual Instruction Tuning (LLaVA)*, arXiv:2304.08485, 2023.
* [4] E. Hu et al., *LoRA: Low-Rank Adaptation of LLMs*, ICLR 2022.
* [5] E. Kolve et al., *AI2-THOR: Interactive 3D Environment for Visual AI*, arXiv:1712.05474, 2017.
* [6] LiteLLM Documentation â€” [https://docs.litellm.ai](https://docs.litellm.ai)

---

## ğŸ‘¨â€ğŸ’» Author

**Rajul Kumar**
Ph.D. Researcher â€” George Mason University
*Humanâ€“Robot Interaction â€¢ Multimodal Learning â€¢ Cognitive Modeling*

ğŸ“§ [rajul.kumar@gmu.edu](mailto:rajul.kumar@gmu.edu)
ğŸ“˜ [LinkedIn](https://www.linkedin.com/in/rajulkumar)
ğŸŒ [Google Scholar](https://scholar.google.com)

---

## ğŸ§¾ License

This project is released under the [MIT License](./LICENSE).

---

## ğŸ§© Citation

If you use this repository, please cite:

```
@article{Kumar2025MFM,
  title   = {Memory-Efficient Multimodal Fusion Model for Visionâ€“Language Robotic Reasoning},
  author  = {Rajul Kumar},
  year    = {2025},
  journal = {GitHub Repository},
  url     = {https://github.com/<your-username>/mfm-robotic-instruction-learning}
}
```

---

```

---

Would you like me to generate this as a **`README.md` file** (so you can directly upload it to your GitHub repository folder)?  
If yes, please confirm your **GitHub username**, and Iâ€™ll embed it in all URLs automatically before generating the file.
```
